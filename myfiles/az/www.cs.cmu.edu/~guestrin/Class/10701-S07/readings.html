<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xmlns="http://www.w3.org/TR/REC-html40"
 xmlns:v="urn:schemas-microsoft-com:vml"
 xmlns:o="urn:schemas-microsoft-com:office:office">
<head>
  <title>10-701 and 15-781 Machine Learning, Spring 2007 - Readings</title>
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <style type="text/css" media="screen"></style>
  <link href="projects_files/filelist.xml" rel="File-List">
<!--[if !mso]> <STYLE>v\:* {
BEHAVIOR: url(#default#VML) }
o\:* { BEHAVIOR: url(#default#VML)
} .shape {
BEHAVIOR: url(#default#VML) }
</STYLE> <![endif]--><!--[if gte mso 9]>
<xml><o:shapedefaults v:ext="edit" spidmax="1027"/>
</xml><![endif]-->
</head>
<body
 style="color: rgb(255, 255, 255); background-color: rgb(0, 0, 40);"
 alink="#00ffff" link="#33ffff" vlink="#ff99ff">
<div align="center">
	<h1><small>Readings - <a href="index.html">10-701 Machine Learning</a></small>
</h1>
</div>
<p>
This page lists the readings for each lecture.  The instructors will include comments and pointers to other resources that might be helpful to get the most out of the readings.

</p>
<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture1">Wed, Jan 17:</a></span></big><br> &nbsp;</p>
<ul>
	<!-- Readings go here -->
	<li>(Bishop - 2.1) This section gives many details on the Bayesian and maximum likelihood results for the binomial example Carlos covered today.
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="recitation1">Recitation 1 -- Probability Review</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop - 1.2) A good review of the probability concepts needed for this course
	<li>We have not checked all of these articles for correctness, but we do recommend brushing up with the Wikipedia articles for these topics:
	<ul>

		<li><a href="http://en.wikipedia.org/wiki/Probability_theory">Probability Theory</a>
		<li><a href="http://en.wikipedia.org/wiki/Probability_axioms">Probability Axioms</a>
		<li><a href="http://en.wikipedia.org/wiki/Random_variable">Random Variable</a>
		<li><a href="http://en.wikipedia.org/wiki/Probability_distribution">Probability Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Statistical_independence">Statistical Independence</a>
		<li><a href="http://en.wikipedia.org/wiki/Conditional_distribution">Conditional Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Marginal_distribution">Marginal Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Joint_distribution">Joint Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Joint_probability">Joint Probability</a>
		<li><a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' Theorem</a>
		<li><a href="http://en.wikipedia.org/wiki/Binomial_distribution">Binomial Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a>
		<li><a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial Distribution</a>
	</ul>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture2">Mon, Jan 22:</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop - 1.1 to 1.4) Introduces curve fitting, reviews probability theory, introduces Gaussians, and covers the famous "curse of dimensionality"
	<li>(Bishop - 3.1, 3.1.1, 3.1.4, 3.1.5, 3.2, 3.3, 3.3.1, 3.3.2) Regression, linear basis function models, bias-variance decomposition, and Bayesian linear regression
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture3">Wed, Jan 24:</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop - 3.2) Bias-variance decomposition
	<li>(Bishop - 1.5.5) Covers loss functions for regression and discusses minimizing expected loss
	<li>(Bishop - 1.3) Discusses model selection using a test set
	<li>Mitchell Chapter (Sections 1 and 2): <a href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Mitchell's Chapter on Naive Bayes and Logistic Regression</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture4">Mon, Jan 29:</a></span></big><br> &nbsp;</p>
<ul>
	<li>Mitchell Chapter (All sections): <a href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Mitchell's Chapter on Naive Bayes and Logistic Regression</a>
	<li>Optional Reading: <a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Handouts/Readings/ng-jordan-2001.ps">Ng and Jordan's NIPS 2001 paper on Discriminative versus Generative Learning</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture5">Wed
, Jan 31:</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop - 14.4) Tree-based Models
	<li>Recommended Reading: Nils Nilsson's Chapter (All Sections): <a href="http://ai.stanford.edu/people/nilsson/MLDraftBook/ch6-ml.pdf">Decision Trees</a>
<li>Optional Review of Boolean Logic/DNF: Nils Nilsson's Chapter 
<a href="http://ai.stanford.edu/people/nilsson/MLDraftBook/ch2-ml.pdf">Boolean
Functions</a> (first 4 pages)
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture7">Wed
, Feb 7:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop - 14.3) Boosting
	<li>Schapire's <a href=
"Handouts/Readings/boosting-schapire.ps">Boosting 
Tutorial</a>
	<li>(Bishop - 1.3) Model Selection (Cross Validation)
</ul>


<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture8">
Mon, Feb 12:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 1.3) Model Selection / Cross Validation
        <li>(Bishop 3.1.4) Regularized least squares
        <li>(Bishop 5.1) Feed-forward Network Functions
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture9">
Wed, Feb 14:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 5.1) Feed-forward Network Functions
	<li>(Bishop 5.2) Network Training
	<li>(Bishop 5.3) Error Backpropagation
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture10">
Wed, Feb 19:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 2.5) Nonparametric Methods
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture11">
Wed, Feb 21:</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop 6.1,6.2) Kernels
	<li>(Bishop 7.1) Maximum Margin Classifiers
	<li><a href="Handouts/Readings/hearst98.pdf">Hearst 1998: High Level Presentation</a>
	<li><a href="http://www.kernel-machines.org/papers/Burges98.ps.gz">Burges 1998: Detailed Tutorial</a>
	<li>Optional Reading: <a href="http://research.microsoft.com/users/jplatt/smo-book.pdf">Platt 1998: Training SVMs with Sequential Minimal Optimization</a>

</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture12">
Mon, Mar 5:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Mitchell Chapter 7) Computational Learning Theory 
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture13">
Wed, Mar 21:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 8.1,8.2) Bayesian Networks, Conditional Independence
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture14">
Mon, Mar 26:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 8.4.1,8.4.2) Inference in Chain/Tree structures
	<li> <a href="http://www.cs.cmu.edu/~guestrin/Class/10701/Handouts/Readings/hmms-rabiner.pdf">Rabiner's HMM tutorial</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture15">
Wed, Mar 28:</a></span></big><br> &nbsp;</p>
<ul>
	<li>Additional Reading:
	<a
	href="ftp://ftp.research.microsoft.com/pub/tr/tr-95-06.pdf">
	Heckerman BN Learning Tutorial</a>
	<li>Additional Reading:
	<a
	href="http://www.cs.huji.ac.il/%7Enir/Papers/FrGG1.pdf">
	Tree-Augmented Naive Bayes paper</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture16">
Mon, Apr 2:</a></span></big><br> &nbsp;</p>
<ul>
        <li>(Bishop 9.1, 9.2) K-means, Mixtures of Gaussians 
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture17">
Wed, Apr 4:</a></span></big><br> &nbsp;</p>
<ul>
	<li><a href="ftp://ftp.cs.utoronto.ca/pub/radford/emk.pdf"> Neal and Hinton EM paper</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture18">
Mon, Apr 9:</a></span></big><br> &nbsp;</p>
<ul>
	<li>(Bishop 9.3, 9.4) EM
</ul>
<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture19">
Wed, Apr 11:</a></span></big><br> &nbsp;</p>
<ul>
	<li><a href="Handouts/Readings/zoubin-hmms.pdf">Ghahramani, "An introduction to HMMs and Bayesian Networks"</a>
</ul>

<p align="left"><br><big><span style="font-weight: bold;"><a name="lecture20">
Mon, Apr 16:</a></span></big><br> &nbsp;</p>
<ul>
	<li><a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/colt98_final.ps">Blum and Mitchell co-training paper</a>
	<li>Optional reading: <a href="http://www.cs.cornell.edu/People/tj/publications/joachims_99c.pdf">Joachims Transductive SVMs paper</a>
</ul>

</body>
</html>
