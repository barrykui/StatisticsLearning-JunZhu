\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}

\title{Assignment 1 for \#70240413 \\ "Statistical Machine Learning" }
\author{Kui XU, 2016311209}
\date{2017/03/23}

\begin{document}

\maketitle

\section{Mathematics Basics}

Choose one problem from the 1.1 and 1.2. A bonus would be given if you finished the both.

\subsection{Calculus}
The gamma function is defined by (assuming x $>$ 0)

\begin{equation}
\Gamma (x) = \int^{\infty}_{0}{u^{x-1}e^{-u}du.}
\end{equation}

\begin{enumerate}
\item[(1)] Prove that $\Gamma (x + 1) =x \Gamma (x)$.
\item[(2)] Also show that 
            \begin{equation}
            \int^{1}_{0}{u^{a-1}{(1-u)}^{b-1}du=\frac{\Gamma(a)\Gamma(b)}{\Gamma (a+b)}.}
            \end{equation}
\end{enumerate}


\textbf{Solution:} For Question (1), we can prove it by Using integration by parts, the steps are as follows:
\begin{equation}
    \begin{aligned}
        \Gamma(x+1) &= \int_0^\infty u^{x} e^{-u}\,du \\
                    &= \left[-u^x e^{-u}\right]_0^\infty + \int_0^\infty x u^{x-1} e^{-u}\, du \\
                    &= \lim_{u\to \infty}(-u^x e^{-u}) - (0 e^{-0}) + x\int_0^\infty u^{x-1} e^{-u}\, du \\
                    &= x\int_0^\infty u^{x-1} e^{-u}\, du\\
                    &= x\Gamma(x)
    \end{aligned}
\end{equation}
As we know, when $u\to \infty$, $-u^x e^{-u}\to 0$, so the equation is proved.\\



\textbf{Solution:} For Question (2), we know that the left of the equation is a Beta function. From the definitions, we can express the equation which we want to prove as : 
\begin{equation}
    \Gamma(a + b) B(a, b)= \frac{\Gamma(a)\Gamma(b)}{\Gamma (a+b)}
\end{equation}
It's a double integral, the expansion formula is as follows:
\begin{equation}
    \begin{aligned}
        \Gamma(a + b) B(a, b)   &= \int_0^\infty u^{a + b -1} e^{-u} du \int_0^1 v^{a-1} (1 - v)^{b-1} dv \\
                                &= \int_0^\infty \int_0^1 (u v)^{a-1}[u (1 - v)]^{b-1} u e^{-u} du \, dv \\
    \end{aligned}
\end{equation}
Then we do a transformation $w = u v$, $z = u (1 - v)$. The inverse transformation is $  u= w + z $, $ v = w \big/(w + z) $ ,  the corresponding ranges of them are $w \in (0, \infty)$ and $u \in (0, \infty)$. \\
The absolute value of the Jacobian is 

\begin{equation}
    \begin{aligned}
        \left|\nabla\frac{\partial(u, v)}{\partial(w, z)}\right| = \frac{1}{(w + z)} 
    \end{aligned}
\end{equation}
Next, we use the changed of variables to do a double integral, the equation above becomes:
\begin{equation}
    \begin{aligned}
        &\int_0^\infty \int_0^\infty w^{a-1} z^{b-1} (w + z) e^{-(w + z)} \frac{1}{w + z} dw \, dz \\
        &= \int_0^\infty \int_0^\infty w^{a-1} z^{b-1} e^{-(w + z)}  dw \, dz \\
        &= \int_0^\infty w^{a-1}e^{-w} dw  \int_0^\infty z^{b-1} e^{- z} dz \\
        &= \Gamma(a)\Gamma(b)
    \end{aligned}
\end{equation}    
Finally the equation is proved.

\subsection{Optimization}

Use the Lagrange multiplier method to solve the following problem:
\begin{equation}
    \begin{aligned}
        \min_{x_1, x_2}    \qquad& x_{1}^{2} + x_{2}^{2} - 1 \\
        s.t.\qquad & x_{1} + x_{2} - 1 =0 \\
        & 2x_{1} - x_{2} \ge 0
    \end{aligned}
\end{equation}    

\textbf{Solution:} Consider the above equation is consist of inequality constraint functions and it is a nonlinear optimization problem, we can use the lagrange multiplier method with KKT condition to solve it. The Objective funtion is
\begin{equation}
    \mathcal{L}(x_1, x_2,\lambda, \mu) = x_{1}^{2} + x_{2}^{2} - 1 + \lambda \cdot \Big(x_{1} + x_{2} - 1\Big) + \mu \cdot \Big(2x_{1} - x_{2}\Big)
\end{equation}    
The certain conditions which are called KKT condition should satisfy,

\begin{equation}
    \begin{aligned}
        \frac{\partial(L)}{\partial(X)}\arrowvert_{X} &=0\\
        \lambda_{j}  \ne 0 \\
        \mu_{k}  \ge 0\\
        \mu_{k} \cdot \Big(x_{1}^{*} + x_{2}^{*} - 1\Big) =0\\
        x_{1}^{*} + x_{2}^{*} - 1 = 0\\
        2x_{1}^{*}  - x_{2}^{*}  \le 0\\
    \end{aligned}
\end{equation}  

Choose one problem from the following 1.3 and 1.4. A bonus would be given if
you finished the both.

\subsection{Stochastic Process}
We toss a fair coin for a number of times and use H(head) and T(tail) to denote the two sides of the coin. Please compute the expected number of tosses we need to observe a first time occurrence of the following consecutive pattern
\begin{equation}
    H,\underbrace{T,T,...,T}_{k}.
\end{equation}

\textbf{Solution:} we asume that $E$ is the expection of the consecutive pattern $H,\underbrace{T,T,...,T}_{k}$, and $E_{T}^{k}$ is the   expection of $\underbrace{T,T,...,T}_{k}$. Consider an equivalent form of this pattern $H,\underbrace{T,T,...,T}_{k-1},T$, we have 
\begin{equation}  
\left\{  
             \begin{array}{lr}  
             E = 1 + \dfrac{1}{2}E +  \dfrac{1}{2}E_{T}^{k}, &  \\  
             E_{T}^{k} = E_{T}^{k-1} + 1 + \dfrac{1}{2}E_{T}^{k} + \dfrac{1}{2} \times 0. &   E_{T}^{1} =2   
             \end{array}  
\right.  
\end{equation}  

which $E = 1 + \dfrac{1}{2}E +  \dfrac{1}{2}E_{T}^{k}$ shows the expection of the first toss. At the first time, you may get $H$ or $T$ with the  $\frac{1}{2}$ probability. If you got $H$, OK, you succuced and then you will try to get $k$ times $T$, the expection will be $\dfrac{1}{2}E_{T}^{k}$; If you got $T$, you fail and will restart to tosses and the expection will be $\dfrac{1}{2}E$. \\
which $ E_{T}^{k} = E_{T}^{k-1} + 1 + \dfrac{1}{2}E_{T}^{k} + \dfrac{1}{2} \times 0$ shows the the expection of the $k-1$ times of $T$ ($E_{T}^{k-1}$) and the last toss. At the last toss, as for the first time, you will get $H$ or $T$ with the  $\frac{1}{2}$ probability. If you got $H$, you fail and you need to get $k$ times $T$ over again and the expection will be $\dfrac{1}{2}E_{T}^{k}$. If you got $T$, OK, you win the game, the expection will be $\dfrac{1}{2}E_{T}^{k}$; 

Next, we solve the recursive function above

\begin{equation}  
    E_{T}^{k} = 2^{k+1} -2
\end{equation} 
$\Rightarrow$

\begin{equation}  
    E = 1 + \dfrac{1}{2}E +  \dfrac{1}{2} (2^{k+1} -2)
\end{equation} 
$\Rightarrow$

\begin{equation}  
    E  =  2^{k+1}
\end{equation} 

So the expected number of tosses is $2^{k+1}$.


\subsection{Probability}

Suppose $p \sim Beta(p|\alpha, \beta)$ and $x|p \sim Bernoulli(x|p)$. Show that $p|x \sim Beta(p|\alpha + x, \beta + 1 - x)$, which implies that the Beta distribution can serve as a conjugate prior to the Bernoulli distribution.

\textbf{Solution:} Consider calculating the posterior $p|x$, and we know the likelihood function $x|p$ and the prior $p$, here we use Bayes' theorem:
\begin{equation}
    \begin{aligned}
        P(p | x) &= \frac{P(x|p) P(p)}{P(x)} \\
                 &= \frac{P(x|p) P(p)}{\int P(x|p') P(p') dp'} 
    \end{aligned}
\end{equation}
From the definition, $P(p) \sim Beta(p|\alpha, \beta)$ and $P(x|p) \sim Bernoulli(x|p)$, and the Beta function is 
\begin{equation}
    Beta(p|\alpha, \beta) = \frac{1}{B(\alpha, \beta)} p^{\alpha - 1} (1-p)^{\beta-1}
\end{equation}

so $P(p | x)$ should be 

\begin{equation}
    \begin{aligned}
        P(p | x) &= \frac{P(x|p) P(p)}{\int_{0}^{1} P(x|p') P(p') dp'}\\
                 &= \frac{ \binom{m}{n}  p^{m} (1-p)^{n-m} \frac{1}{B(\alpha, \beta)} p^{\alpha - 1} (1-p)^{\beta-1}}{\int_{0}^{1} \binom{m}{n}  p^{m} (1-p)^{n-m} \frac{1}{B(\alpha, \beta)} p^{\alpha - 1} (1-p)^{\beta-1} dp}\\
                 &= \frac{  p^{\alpha + m - 1} (1-p)^{\beta-1+n-m}}{\int_{0}^{1} p^{\alpha +m - 1} (1-p)^{\beta-1+n-m} dp}\\
                 &=  \frac{  p^{\alpha + m - 1} (1-p)^{\beta-1+n-m}}{B(\alpha + m, \beta+n-m )}\\
                 &= Beta(p|\alpha + m, \beta+n-m)
    \end{aligned}
\end{equation}
So, it implies that the Beta distribution can serve as a conjugate prior to the Bernoulli distribution.
















Differentiation allows for the calculation of the slope of the tangent of a curve at any given point as shown in Figure \ref{exampleplot}.

\begin{figure}[!htbp]
\begin{center}
\begin{tikzpicture}
\draw[domain=-2:2, color=blue] plot (\x, {1 - (\x)^2}) node[above = .5cm, right, color=blue] {$f(x)=1-x^2$};
\draw[domain=-2:2, color=red] plot(\x,-1 * \x + 1.25) node[above = .5cm, right, color=red] {Tangent at $x=.5$};
\draw [thick, ->] (-3,0) -- (3,0) node [above] {$x$};
\draw [thick, ->] (0,-3) -- (0,3) node [right] {$y$};
\node at (.5,.75) {\textbullet};
\end{tikzpicture}
\end{center}
\caption{The plot of $f(x)=1-x^2$ with a tangent at $x=.5$.}\label{exampleplot}
\end{figure}

Differentiation is now a technique taught to mathematics students throughout the world. In this document I will discuss some aspects of differentiation.

\section{Exploring the derivative using Sage}

The definition of the limit of $f(x)$ at $x=a$ denoted as $f'(a)$ is:

\begin{equation}
f'(a) = \lim_{h\to0}\frac{f(a+h)-f(a)}{h}
\end{equation}

The following code can be used in sage to give the above limit:

\begin{verbatim}
def illustrate(f, a):
    """
    Function to take a function and illustrate the limiting definition of a derivative at a given point.
    """
    lst = []
    for h in srange(.01, 3, .01):
        lst.append([h,(f(a+h)-f(a))/h])
    return list_plot(lst, axes_labels=['$x$','$\\frac{f(%.02f+h)-f(%.02f)}{h}$' % (a,a)])
\end{verbatim}

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[width=8cm]{sage1.png}
\end{center}
\caption{The derivative of $f(x)=1-x^2$ at $x=.5$ converging to -1 as $h\to0$.}
\end{figure}

If we want to plot the tangent at a point $\alpha$ to a function we can use the following:

\begin{align}
y=&ax+b&&\text{(definition of a straight line)}\nonumber\\
  &f'(a)x+b&&\text{(definition of the derivative)}\nonumber\\
  &f'(a)x+f(a)-f'(a)a&&\text{(we know that the line intersects $f$ at $(a,f(a))$}\nonumber
\end{align}

We can combine this with the approach of the previous piece of code to see how the tangential line converges as the limiting definition of the derivative converges:

\begin{verbatim}
def convergetangentialline(f, a, x1, x2, nbrofplots=50, epsilon=.1):
    """
    Function to make a tangential line converge
    """
    clrs = rainbow(nbrofplots)
    k = 0
    h = epsilon
    p = plot(f, x, x1, x2)
    while k < nbrofplots:
        tangent(x) = fdash(f, a, h) * x + f(a) - fdash(f, a, h) * a
        p += plot(tangent(x), x, x1, x2, color=clrs[k])
        h += epsilon
        k += 1
    return p
\end{verbatim}

The plot shown in Figure \ref{lines} shows how the lines shown converge to the actual tangent to $1-x^2$ as $x=2$ (the red line is the `closest' curve).

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[width=8cm]{sage0.png}
\end{center}
\caption{Lines converging to the tangent curve as $h\to0$.}\label{lines}
\end{figure}

Note here that the last plot is given using the \textbf{real} definition of the derivative and not the approximation.

\section{Conclusions}

In this report I have explored the limiting definition of the limit showing how as $h\to 0$ we can visualise the derivative of a function. The code involved \url{https://sage.maths.cf.ac.uk/home/pub/18/} uses the differentiation capabilities of Sage but also the plotting abilities.

There are various other aspects that could be explored such as symbolic differentiation rules. For example:

$$\frac{dx^n}{dx}=(n+1)x^{n}\text{ if }x\ne-1$$

Furthermore it is interesting to not that there exists some functions that \textbf{are not} differentiable at a point such as the function $f(x)=\sin(1/x)$ which is not differentiable at $x=0$. A plot of this function is shown in Figure \ref{notdiff}.

\begin{figure}[!htbp]
\begin{center}
%\includegraphics[width=8cm]{sage2.png}
\end{center}
\caption{None differentiable function at $x=0$.}\label{notdiff}
\end{figure}


\bibliographystyle{plain}
%\bibliography{bibliography.bib}
\end{document}